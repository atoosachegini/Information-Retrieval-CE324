{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-b2UsRU4P9JF"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;\">\n",
    "<font face=\"XB Zar\" size=5>\n",
    "<div align=center>\n",
    "<font face=\"B Titr\" size=5>\n",
    "<p></p><p></p>\n",
    "بسمه تعالی\n",
    "<p></p>\n",
    "</font>\n",
    "<p></p>\n",
    "<font>\n",
    "<br>\n",
    "درس بازیابی پیشرفته اطلاعات\n",
    "<br>\n",
    "مدرس: دکتر سلیمانی\n",
    "</font>\n",
    "<p></p>\n",
    "<br>\n",
    "<font>\n",
    "<b>فاز اول پروژه</b>\n",
    "</font>\n",
    "<br>\n",
    "<br>\n",
    "موعد تحویل: ۲۸ اسفند ۱۳۹۹\n",
    "<br>\n",
    "<br>\n",
    "<font size=4.8>\n",
    "دستیاران آموزشی: نیما جمالی، آرمین سعادت، ایمان غلامی\n",
    "</font>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<font>\n",
    "دانشگاه صنعتی شریف\n",
    "<br>\n",
    "دانشکده مهندسی کامپیوتر\n",
    "<br>\n",
    "<br>\n",
    "</font>\n",
    "</div>\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y3YoNVsQR3zO"
   },
   "source": [
    "<p></p>\n",
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h1>\n",
    "    <b>مقدمه</b>\n",
    "    </h1>\n",
    "    <p></p>\n",
    "    <p></p>\n",
    "    هدف از فاز اول پروژه، طراحی و پیاده‌سازی سیستم بازیابی اطلاعات برای مجموعه دادگان تعیین شده می‌باشد.<br>\n",
    "    اولین مرحله، پیش‌پردازش مجموعه دادگان است. پس از آن نمایه‌ها با ویژگی‌های خواسته شده پیاده‌سازی می‌شود. در گام بعدی به ذخیره و بازخوانی نمایه‌ها به همراه روش‌های فشرده‌سازی پرداخته می‌شود. پس از آن تکنیک‌های اصلاح پرسمان پیاده‌سازی شده و در نهایت جستجو روی دادگان صورت می‌گیرد. همچنین در بخش آخر با پیاده‌سازی برخی معیارهای ارزیابی، عملکرد سیستم مورد سنجش قرار می‌گیرد.<br><br>\n",
    "     توضیحات مربوط به هر بخش در ادامه آمده است که اهداف، محدودیت‌ها و خواسته‌های آن بخش را مشخص می‌کند.\n",
    "     در هر بخش توابعی مشخص شده است که محتوای آن با کد نوشته شده توسط شما باید پر شود. شما می‌توانید در همین فایل، سیستم بازیابی خود را پیاده‌سازی کنید یا فایل‌های خودتان را ایمپورت کرده و از آن‌ها استفاده کنید. در هر صورت تمامی کدهای لازم را به همراه این نوت‌بوک ارسال کنید. همچنین در نظر داشته باشید که ملاک اصلی نمره‌دهی شما اجرای صحیح توابع مشخص‌شده در این نوت‌بوک می‌باشد. بنابراین از صحت اجرای نوت‌بوک خود اطمینان پیدا کنید.<br><br>\n",
    "     تنها زبان قابل قبول برای پروژه پایتون است. محدودیت استفاده از کتاب‌خانه‌های آماده در هر بخش مشخص شده است. پروژه ۱۱۵ نمره دارد که ۱۵ نمره از آن امتیازی و مربوط به بخش اصلاح پرسمان می‌باشد.\n",
    "    \n",
    "    \n",
    "</font>\n",
    "</div>\n",
    "<p></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jaR5CS_khMQB"
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h1>\n",
    "    <b>مجموعه دادگان</b>\n",
    "    </h1>\n",
    "    مجموعه دادگان مورد بررسی در این پروژه از سایت kaggle فراهم شده است. این مجموعه شامل اطلاعات ۶۰۰۰ فیلم سینمایی از سال ۱۹۰۴ تا ۲۰۱۷ است. داده‌ها در قالب فایل csv   دارای ستون‌‌های plot، title، id می‌باشد. id یک شناسه یکتا برای هر فیلم است که برای ارزیابی بهتر عملکرد شما به داده‌ها اضافه شده است و در مجموعه دادگان اصلی وجود نداشته است. همانطور که می‌دانید برای پیاده‌سازی نمایه باید به هر داکیومنت یک شناسه اختصاص بدهید. <b>شناسه مربوط به هر داکیومنت باید id ذکر شده برای آن در مجموعه دادگان باشد.</b> هر فیلم از دو بخش title و plot  تشکیل شده است که از این دو بخش در ساخت نمایه و جستجو استفاده می‌شود. plot خلاصه‌ای از طرح داستان فیلم است.<br>\n",
    "    علاوه بر مجموعه دادگان اصلی، تعدای پرسمان در اختیار شما قرار گرفته است. همچنین جواب مطلوب هر یک از این پرسمان‌ها نیز فراهم شده که طبیعتا زیرمجموعه‌ای از مجموعه دادگان است. شما باید از این پرسمان‌ها و نتیجه مورد انتظار هر کدام برای ارزیابی سامانه خود استفاده کنید.\n",
    "    پرسمان‌ها و شناسه فیلم‌های بازیابی شده مورد انتظار از هر پرسمان در فایل validation.json آمده است. توضیحات بیشتر در رابطه با استفاده از این فایل در بخش ارزیابی آمده است.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PxNntHofmHHH"
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h1>\n",
    "    <b>پیش‌پردازش و آماده‌سازی داده‌ها (۱۵ نمره)</b>\n",
    "    </h1>\n",
    "    در این بخش ابتدا داده‌ها را از فایل بخوانید. برای آماده‌سازی متن می‌توانید از کتاب‌خانه‌های آماده استفاده کنید. یکی از کتاب‌خانه‌های معروف برای این کار <a href=\"https://www.nltk.org/\">NLTK</a> است اما در انتخاب روش پیاده‌سازی این بخش مختارید. برای این بخش باید تابع ()prepare_text را تکمیل کنید. این تابع یک متن انگلیسی ورودی گرفته و توکن‌‌های مربوط به آن‌را در قالب یک لیست خروجی می‌دهد. متن ورودی در عمل تایتل یا طرح داستان هر فیلم است. دقت کنید که لیست خروجی شامل تعدادی توکن است که عملیات case folding، stemming و lemmatization روی آن‌ها اجرا شده است. در ضمن علائم نگارشی نباید به عنوان توکن در نظر گرفته شود. در کد زیر یک نمونه ورودی و خروجی نمایش داده شده است. با توجه به نحوه پیاده‌سازی انواع بازگردانی به ریشه قابل قبول است.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9aa16Puk-if0",
    "outputId": "2e6a2c95-41d5-4146-a27b-5ceba034e291"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': ['the', 'suburbanit'], 'plot': ['the', 'film', 'be', 'about', 'a', 'famili', 'who', 'move', 'to', 'the', 'suburb', 'hop', 'for', 'a', 'quiet', 'life', 'thing', 'start', 'to', 'go', 'wrong', 'and', 'the', 'wife', 'get', 'violent', 'and', 'start', 'throw', 'crockeri', 'lead', 'to', 'her', 'arrest']}\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import hazm\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "lem = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "pers_norm = hazm.Normalizer()\n",
    "pers_stem = hazm.Stemmer()\n",
    "\n",
    "def simple_dct(doc):\n",
    "    simple_di = dict()\n",
    "    for k, v in doc.items():\n",
    "        tokens = v.split()\n",
    "        valid_tokens = []\n",
    "        for token in tokens:\n",
    "            #case_folding by .lower\n",
    "            norm_token = token.lower()\n",
    "            if norm_token in {\",\", \":\", \"!\", \"(\", \")\", \"«\", \"»\", \"?\", \";\"}:   \n",
    "                valid_tokens[:] = (value for value in  tokens if value != token)\n",
    "            if norm_token:\n",
    "                valid_tokens.append(norm_token) \n",
    "        simple_di[k] = valid_tokens\n",
    "    return simple_di\n",
    "\n",
    "def parse_csv(csvfile, col_list=['plot', 'title']):\n",
    "    df = pd.read_csv(csvfile, usecols=col_list)\n",
    "    return df.to_dict(orient='records')\n",
    "\n",
    "def lemma(word):\n",
    "    res = lem.lemmatize(word, pos='v')\n",
    "    if res == word:\n",
    "        res = lem.lemmatize(word, pos='n')\n",
    "    return res\n",
    "\n",
    "\n",
    "def stem(word):\n",
    "    return stemmer.stem(word)\n",
    "\n",
    "def prepare_text(raw_text):\n",
    "    #tokenization\n",
    "    tokens = word_tokenize(raw_text)\n",
    "    valid_tokens = []\n",
    "    big_tokens = []\n",
    "    for token in tokens:\n",
    "        #case_folding by .lower\n",
    "        norm_token = re.sub(r'[^\\w\\s]', '', token.lower())\n",
    "        if norm_token:\n",
    "            # stemming and lemmatization by def defined above\n",
    "            valid_tokens.append(stem(lemma(norm_token)))\n",
    "    return valid_tokens\n",
    "\n",
    "def prepare_doc(doc):\n",
    "    res = dict()\n",
    "    for k, v in doc.items():\n",
    "        res[k] = prepare_text(v)\n",
    "    return res\n",
    "\n",
    "\n",
    "#print(prepare_text(\"exponential I AM retrieval\"))\n",
    "docs = parse_csv('movies.csv')\n",
    "sim_dic = list(map(simple_dct, docs))\n",
    "#print(len(docs))\n",
    "prepared_docs = list(map(prepare_doc, docs))\n",
    "#testing\n",
    "print(prepared_docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WVhVEO6VARIa"
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h1>\n",
    "    <b>شناسایی و حذف stop-words (۵ نمره)</b>\n",
    "    </h1>\n",
    "    این بخش باید توسط خودتان و بدون استفاده از کد آماده پیاده‌سازی شود. ترم‌های موجود در مجموعه دادگان را بر اساس تکرار آن‌ها مرتب کرده و پرتکرارترین آن‌ها را به عنوان stop-words در نظر بگیرید. اینکه چند ترم را به عنوان stop-words در نظر بگیرید به عهده خودتان است.<br>\n",
    "    با فراخوانی تابع ()get_stop_words لیست stop-words به همراه تعداد تکرار آن‌‌ها خروجی داده می‌شود.<br>\n",
    "    ترم‌های به دست آمده از این بخش نباید در نمایه حضور داشته باشند.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 147381, 'to': 91568, 'and': 83092, 'a': 67783, 'of': 42764, 'is': 40362, 'in': 38565, 'his': 34702, 'he': 32319, 's': 30898, 'her': 27941, 'that': 26292, 'with': 25052, 'him': 18921, 'by': 17818, 'for': 17583, 'she': 17576, 'as': 15617, 'on': 14846, 'but': 13508, 'they': 13351, 'who': 12725, 'at': 12337, 'from': 11095, 'has': 11003, 'an': 10855, 'when': 9990, 'their': 9475, 'are': 9242, 'it': 9082, 'after': 8876, 'out': 7819, 'into': 7323, 'up': 6776, 'be': 6571, 'them': 6554, 'not': 5890, 'was': 5348, 'while': 5234, 'one': 5215, 'then': 4967, 'which': 4590, 'will': 4507, 'about': 4486, 'where': 4408, 'have': 4336, 'back': 4323, 'two': 3929, 'tells': 3796, 'new': 3764}\n"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter \n",
    "\n",
    "def parse_csv2(csvfile, col_list=['plot', 'title']):\n",
    "    doc_texts = []\n",
    "    df = pd.read_csv(csvfile, usecols=col_list)\n",
    "    for idx, row in df.iterrows():\n",
    "        doc_texts.append(\" \".join([row[col] for col in col_list]))\n",
    "    return doc_texts\n",
    "\n",
    "def extract_common_words(cleaned_docs, number=50):\n",
    "    all_words = set(word for doc in cleaned_docs for word in doc)\n",
    "    counts = dict.fromkeys(all_words, 0)\n",
    "    for doc in cleaned_docs:\n",
    "        for word in doc:\n",
    "            counts[word] += 1\n",
    "    sorted_counts = sorted(counts.items(), key = itemgetter(1), reverse = True)[:number]\n",
    "    return sorted_counts\n",
    "\n",
    "def get_stop_words():\n",
    "    doc_texts = parse_csv2(\"movies.csv\")\n",
    "    cleaned_docs = []\n",
    "    for text in doc_texts:\n",
    "        tokens = word_tokenize(text)\n",
    "        valid_tokens = []\n",
    "        for token in tokens:\n",
    "            norm_token = re.sub(r'[^\\w\\s]', '', token.lower())\n",
    "            if norm_token:\n",
    "                valid_tokens.append(norm_token)\n",
    "        cleaned_docs.append(valid_tokens)\n",
    "    stop_words = dict(extract_common_words(cleaned_docs))\n",
    "    return stop_words\n",
    "\n",
    "stop_words = get_stop_words()\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': ['suburbanit'], 'plot': ['film', 'famili', 'move', 'suburb', 'hop', 'quiet', 'life', 'thing', 'start', 'go', 'wrong', 'wife', 'get', 'violent', 'start', 'throw', 'crockeri', 'lead', 'arrest']}\n"
     ]
    }
   ],
   "source": [
    "set_stw = set(stop_words)\n",
    "updated_dictionary = []\n",
    "def delete_stw(dic):\n",
    "    updated = dict()\n",
    "    updated[\"title\"] = []\n",
    "    updated[\"plot\"] = []\n",
    "    for i in dic.get(\"title\"):\n",
    "        if i not in set_stw:\n",
    "            updated[\"title\"].append(i)\n",
    "    for i in dic.get(\"plot\"):\n",
    "        if i not in set_stw:\n",
    "            updated[\"plot\"].append(i)\n",
    "    return updated\n",
    "#we should now remove these stopwords from prepared_docs\n",
    "for i in range(0, 6000):\n",
    "    updated_dictionary.append(delete_stw(prepared_docs[i]))\n",
    "print(updated_dictionary[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A4Z02BzNHD1z"
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h1>\n",
    "    <b>نمایه‌سازی (۱۵ نمره)</b>\n",
    "    </h1>\n",
    "    در این بخش باید برای سامانه positional index بسازید. برای هر ترم باید مشخص باشد که آن ترم در تایتل چه فیلم‌هایی و در چه جایگاهی از تایتل هر فیلم قرار گرفته است. همچنین برای هر ترم باید مشخص باشد که آن ترم در طرح داستان چه فیلم‌هایی و در چه جایگاهی از طرح داستان هر فیلم قرار گرفته است.<br>\n",
    "     برای ارزیابی بهتر و عادلانه‌تر به ویژه برای ارزیابی بخش فشرده‌سازی، از استاندارد بیان شده در قطعه کد زیر برای ذخیره posting list هر ترم در RAM استفاده کنید.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = {}\n",
    "docs_set = {}\n",
    "raw_docs = {}\n",
    "# test\n",
    "def create_index_for_a_doc(doc, doc_id):\n",
    "        global index\n",
    "        global docs_set\n",
    "        global raw_docs\n",
    "        docs_set[doc_id] = []\n",
    "        raw_docs[doc_id] = doc\n",
    "        all_terms = []\n",
    "        for header, term_list in doc.items():\n",
    "            all_terms += [(term, header, i) for i, term in enumerate(term_list)]\n",
    "        for (term, header, i) in all_terms:\n",
    "            if not term in index:\n",
    "                index[term] = []\n",
    "                index[term].append([])\n",
    "                index[term].append([])\n",
    "            tmp = index[term]\n",
    "            if header == 'title':\n",
    "                flag = True\n",
    "                for j in range(len(tmp[0])):\n",
    "                    if doc_id == tmp[0][j][0]:\n",
    "                        flag = False\n",
    "                        if not i in tmp[0][j][1]:\n",
    "                            tmp[0][j][1].append(i)\n",
    "                if flag == True:\n",
    "                    tmp[0].append([doc_id, [i]])\n",
    "            if header == 'plot':\n",
    "                flag = True\n",
    "                for j in range(len(tmp[1])):\n",
    "                    if doc_id == tmp[1][j][0]:\n",
    "                        flag = False\n",
    "                        if not i in tmp[1][j][1]:\n",
    "                            tmp[1][j][1].append(i)\n",
    "                if flag == True:\n",
    "                    tmp[1].append([doc_id, [i]])\n",
    "def create_positional_index():\n",
    "    cur_doc_id = 0\n",
    "    for doc in updated_dictionary:\n",
    "        create_index_for_a_doc(doc, cur_doc_id)\n",
    "        cur_doc_id += 1\n",
    "\n",
    "create_positional_index()\n",
    "print(index['night'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "voqMrk4rg1qk"
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h1>\n",
    "    <b>پویا‌سازی نمایه (۱۰ نمره)</b>\n",
    "    </h1>\n",
    "    نمایه ایجاد شده باید قابلیت حذف و اضافه تک داکیومنت را داشته باشد.\n",
    "    برای اضافه شدن داکیومنت، به تابع ()add_single_document یک رشته داده می‌شود که اطلاعات مربوط به داکیومنت شامل id و plot و title در آن با کاما جدا شده است. برای حذف داکیومنت نیز id آن به تابع ()remove_single_document داده می‌شود.<br>\n",
    "    تضمین می‌شود که شرط یکتا بودن id داکیومنت‌ها نقض نشود. برای مثال دو داکیومنت با شناسه یکسان به مجموعه اضافه نخواهد شد. البته ممکن است حذف شده و دوباره اضافه شود.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "e1Ej2n3MB6ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[38, [1]], [853, [2]], [1137, [1]], [1223, [1]], [1224, [1]], [1225, [1]], [1391, [1]], [5596, [1]], [1234, [2]]], [[38, [81, 90]], [119, [167]], [187, [21]], [242, [50]], [335, [48]], [424, [15]], [524, [5]], [537, [62]], [556, [14]], [761, [130]], [853, [3, 18, 46, 80, 96, 142, 159, 173, 189]], [857, [1, 46, 72, 121, 165, 209, 252, 262, 271, 272, 290, 310]], [1024, [289]], [1137, [5, 15, 26, 28, 34, 40, 45, 73, 86, 98, 129, 190, 209, 217, 225, 245, 272, 281, 315, 339]], [1223, [1, 22, 26, 40, 58, 81, 133, 140, 153, 168, 186, 200, 221, 262, 298, 307, 321, 329, 335, 365]], [1224, [61, 72, 90, 142, 196, 206, 216, 227, 238, 254, 299]], [1225, [17, 37, 53, 57, 101, 122, 133, 144, 159, 174]], [1330, [16, 23, 37, 67, 77, 97, 118, 123, 132, 139]], [1334, [3, 26, 50, 63, 83, 91, 102, 114, 133, 149, 157, 163, 172, 187]], [1386, [4, 10, 33, 46, 52, 70, 140, 150, 162]], [1391, [0, 84, 86, 104, 133, 166, 182]], [1416, [5, 9, 42, 50, 70, 100, 105, 134, 137, 152, 170, 172, 197, 202, 220]], [1446, [42, 52, 63]], [1497, [173]], [1686, [14]], [1735, [28]], [2482, [35, 36, 51, 67, 82, 105, 136, 139, 158, 166, 186]], [2497, [34]], [2585, [60, 63, 72, 80, 97, 111, 118, 153]], [2659, [108, 412]], [2791, [11]], [3234, [12, 32, 68, 102]], [3416, [11, 33, 43, 55, 67]], [3435, [96, 249]], [4894, [59]], [5172, [14]], [5256, [141]], [5330, [49]], [5595, [94]], [5596, [4, 46, 54, 68, 82, 116, 134, 160, 216, 220, 240, 258, 305, 314, 343, 350, 359, 381]], [5903, [275]], [5916, [8, 212, 367, 371]], [5954, [12]], [1234, [3]]]]\n"
     ]
    }
   ],
   "source": [
    "def add_single_document(document):\n",
    "    doc_id, title, plot = document.split(\",\")\n",
    "    doc = {}\n",
    "    doc_id = int(doc_id)\n",
    "    doc[\"title\"] = title\n",
    "    doc[\"plot\"] = plot\n",
    "    result = delete_stw(prepare_doc(doc))\n",
    "    create_index_for_a_doc(result, doc_id)\n",
    "    \n",
    "\n",
    "new_document = \"1234,The Adventures of Sherlock Holmes,The picture begins with Moriarty and Holmes verbally sparring on the steps\"\n",
    "add_single_document(new_document)\n",
    "print(index[\"holm\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "DsarzVzhDoMi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[38, [1]], [853, [2]], [1137, [1]], [1223, [1]], [1224, [1]], [1225, [1]], [1391, [1]], [5596, [1]]], [[38, [81, 90]], [119, [167]], [187, [21]], [242, [50]], [335, [48]], [424, [15]], [524, [5]], [537, [62]], [556, [14]], [761, [130]], [853, [3, 18, 46, 80, 96, 142, 159, 173, 189]], [857, [1, 46, 72, 121, 165, 209, 252, 262, 271, 272, 290, 310]], [1024, [289]], [1137, [5, 15, 26, 28, 34, 40, 45, 73, 86, 98, 129, 190, 209, 217, 225, 245, 272, 281, 315, 339]], [1223, [1, 22, 26, 40, 58, 81, 133, 140, 153, 168, 186, 200, 221, 262, 298, 307, 321, 329, 335, 365]], [1224, [61, 72, 90, 142, 196, 206, 216, 227, 238, 254, 299]], [1225, [17, 37, 53, 57, 101, 122, 133, 144, 159, 174]], [1330, [16, 23, 37, 67, 77, 97, 118, 123, 132, 139]], [1334, [3, 26, 50, 63, 83, 91, 102, 114, 133, 149, 157, 163, 172, 187]], [1386, [4, 10, 33, 46, 52, 70, 140, 150, 162]], [1391, [0, 84, 86, 104, 133, 166, 182]], [1416, [5, 9, 42, 50, 70, 100, 105, 134, 137, 152, 170, 172, 197, 202, 220]], [1446, [42, 52, 63]], [1497, [173]], [1686, [14]], [1735, [28]], [2482, [35, 36, 51, 67, 82, 105, 136, 139, 158, 166, 186]], [2497, [34]], [2585, [60, 63, 72, 80, 97, 111, 118, 153]], [2659, [108, 412]], [2791, [11]], [3234, [12, 32, 68, 102]], [3416, [11, 33, 43, 55, 67]], [3435, [96, 249]], [4894, [59]], [5172, [14]], [5256, [141]], [5330, [49]], [5595, [94]], [5596, [4, 46, 54, 68, 82, 116, 134, 160, 216, 220, 240, 258, 305, 314, 343, 350, 359, 381]], [5903, [275]], [5916, [8, 212, 367, 371]], [5954, [12]]]]\n"
     ]
    }
   ],
   "source": [
    "def remove_single_document(document_id):\n",
    "    for term in index:\n",
    "        for m in index[term][0]:\n",
    "            if document_id == m[0]:\n",
    "                index[term][0].remove(m)\n",
    "        for m in index[term][1]:\n",
    "            if document_id == m[0]:\n",
    "                index[term][1].remove(m)\n",
    "remove_single_document(1234)\n",
    "print(index['holm'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4PBbFcmpXFKD"
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h1>\n",
    "    <b>ذخیره و فشرده‌سازی نمایه (۲۰ نمره)</b>\n",
    "    </h1>\n",
    "    در این بخش باید توانایی ذخیره کردن نمایه و بارگذاری مجدد آن را به سامانه اضافه کنید. ذخیره‌سازی به ۳ روش صورت می‌گیرد. بدون فشرده‌سازی، فشرده‌سازی از روش gamma-code و فشرده‌سازی از روش variable-byte. روش‌های فشرده‌سازی باید توسط خودتان پیاده‌سازی شود.\n",
    "    برای ذخیره نمایه در فایل نیز از JSON  استفاده کنید.<br>\n",
    "     بخشی از نمره شما در این قسمت به میزان فشرده‌سازی نمایه اختصاص داده شده است. بنابراین پیاده‌سازی بهینه روش‌های فشرده‌سازی مهم است.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zYN9I4_BD178",
    "outputId": "4a11351f-d214-495e-eb3f-7dc56863cd58"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12882445\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from struct import pack, unpack\n",
    "from math import log\n",
    "import json\n",
    "from functools import reduce\n",
    "\n",
    "from __future__ import division \n",
    "from struct import pack, unpack\n",
    "\n",
    "def vb_encode_number(number):\n",
    "    bytes_list = []\n",
    "    while True:\n",
    "        bytes_list.insert(0, number % 128)\n",
    "        if number < 128:\n",
    "            break\n",
    "        number = number // 128\n",
    "    bytes_list[-1] += 128\n",
    "    return pack('%dB' % len(bytes_list), *bytes_list)\n",
    "\n",
    "def vb_encode(numbers):\n",
    "    bytes_list = []\n",
    "    for number in numbers:\n",
    "        bytes_list.append(vb_encode_number(number))\n",
    "    return b\"\".join(bytes_list)\n",
    "\n",
    "def vb_decode(bytestream):\n",
    "    n = 0\n",
    "    numbers = []\n",
    "    bytestream = unpack('%dB' % len(bytestream), bytestream)\n",
    "    for byte in bytestream:\n",
    "        if byte < 128:\n",
    "            n = 128 * n + byte\n",
    "        else:\n",
    "            n = 128 * n + (byte - 128)\n",
    "            numbers.append(n)\n",
    "            n = 0\n",
    "    return numbers\n",
    "def gamma_encode_number(number):\n",
    "    val = bin(number)\n",
    "    val = str(val)[3:]\n",
    "    length = len(val)\n",
    "    st = \"\"\n",
    "    for i in range(0, length):\n",
    "        st += \"1\"\n",
    "    st += \"0\"\n",
    "    st = st + val\n",
    "    return st\n",
    "def gamma_encode(numbers):\n",
    "    res = \"\"\n",
    "    numberss = []\n",
    "    for i in range(len(numbers)):\n",
    "        numberss.append(numbers[i])\n",
    "    for i in range(0, len(numberss)):\n",
    "        numberss[i] += 1\n",
    "    for i in range(1, len(numberss)):\n",
    "        numberss[i] = numberss[i] - numberss[i-1]\n",
    "    for number in numberss:\n",
    "        res += gamma_encode_number(number)\n",
    "    return res\n",
    "def gamma_decode(gamma):\n",
    "    numbers = []\n",
    "    i = 0\n",
    "    while i < len(gamma):\n",
    "        if gamma[i] == '0':\n",
    "            numbers.append(1)\n",
    "            i += 1\n",
    "        else:\n",
    "            length = 0\n",
    "            while gamma[i] == '1':\n",
    "                i += 1\n",
    "                length += 1\n",
    "            if gamma[i] == \"0\":\n",
    "                i += 1\n",
    "                number = gamma[i: i + length]\n",
    "                number  = \"1\" + number\n",
    "                numbers.append(int(number, 2))\n",
    "                i += length\n",
    "    j = len(numbers)-1\n",
    "    while j >= 1:\n",
    "        numbers[j] = numbers[j] + numbers[j-1]\n",
    "        j -=1\n",
    "    for i in range(0, len(numbers)):\n",
    "        numbers[i] -= 1\n",
    "    return numbers\n",
    "\n",
    "def store_index(path, compression_type):\n",
    "    index_size = 0\n",
    "    compressed_index = {}\n",
    "    if compression_type == \"no-compression\":\n",
    "        index_size = len(json.dumps(index))\n",
    "        with open(path, 'w') as fp:\n",
    "            fp.write(json.dumps(index))\n",
    "    elif compression_type == \"variable-byte\":\n",
    "        for term in index:\n",
    "            compressed_index[term] = []\n",
    "            compressed_index[term].append([])\n",
    "            compressed_index[term][0].append([])\n",
    "            compressed_index[term][0].append([])\n",
    "            compressed_index[term].append([])\n",
    "            compressed_index[term][1].append([])\n",
    "            compressed_index[term][1].append([])\n",
    "            for i in range(len(index[term][0])):\n",
    "                compressed_index[term][0][0].append(index[term][0][i][0])\n",
    "                compressed_index[term][0][1].append(vb_encode(index[term][0][i][1]).hex())\n",
    "                \n",
    "            for j in range(len(index[term][1])):\n",
    "                compressed_index[term][1][0].append(index[term][1][j][0])\n",
    "                compressed_index[term][1][1].append(vb_encode(index[term][1][j][1]).hex())\n",
    "                \n",
    "#             compressed_index[term][0][0] = int.from_bytes(vb_encode(compressed_index[term][0][0]), byteorder=\"big\")\n",
    "#             compressed_index[term][1][0] = int.from_bytes(vb_encode(compressed_index[term][1][0]), byteorder=\"big\")\n",
    "\n",
    "        compressed_index = str(compressed_index).replace(\" \", \"\")\n",
    "        index_size = len(json.dumps(compressed_index))\n",
    "        com_ind = json.dumps(compressed_index)\n",
    "        with open(path, 'w') as fp:\n",
    "            fp.write(com_ind)\n",
    "            \n",
    "    if compression_type == \"gamma-code\":\n",
    "        for term in index:\n",
    "            compressed_index[term] = []\n",
    "            compressed_index[term].append([])\n",
    "            compressed_index[term][0].append([])\n",
    "            compressed_index[term][0].append([])\n",
    "            compressed_index[term].append([])\n",
    "            compressed_index[term][1].append([])\n",
    "            compressed_index[term][1].append([])\n",
    "            for i in range(len(index[term][0])):\n",
    "                compressed_index[term][0][0].append(index[term][0][i][0])\n",
    "                compressed_index[term][0][1].append(int(gamma_encode(index[term][0][i][1]), 2))\n",
    "                \n",
    "            for j in range(len(index[term][1])):\n",
    "                compressed_index[term][1][0].append(index[term][1][j][0])\n",
    "                compressed_index[term][1][1].append(int(gamma_encode(index[term][1][j][1]), 2))\n",
    "#             compressed_index[term][0][0] = gamma_encode(np.array(compressed_index[term][0][0]))[0]\n",
    "#             compressed_index[term][1][0] = gamma_encode(np.array(compressed_index[term][1][0]))[0]\n",
    "\n",
    "        compressed_index = str(compressed_index).replace(\" \", \"\")\n",
    "        index_size = len(json.dumps(compressed_index))\n",
    "        com_ind = json.dumps(compressed_index)\n",
    "        with open(path, 'w') as fp:\n",
    "            fp.write(com_ind)\n",
    "    return index_size\n",
    "print(store_index(\"gamma_compressed_index.json\", \"gamma-code\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "3Dm38ZXHFQVH",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[38, [1]], [853, [2]], [1137, [1]], [1223, [1]], [1224, [1]], [1225, [1]], [1391, [1]], [5596, [1]]], [[38, [81, 90]], [119, [167]], [187, [21]], [242, [50]], [335, [48]], [424, [15]], [524, [5]], [537, [62]], [556, [14]], [761, [130]], [853, [3, 18, 46, 80, 96, 142, 159, 173, 189]], [857, [1, 46, 72, 121, 165, 209, 252, 262, 271, 272, 290, 310]], [1024, [289]], [1137, [5, 15, 26, 28, 34, 40, 45, 73, 86, 98, 129, 190, 209, 217, 225, 245, 272, 281, 315, 339]], [1223, [1, 22, 26, 40, 58, 81, 133, 140, 153, 168, 186, 200, 221, 262, 298, 307, 321, 329, 335, 365]], [1224, [61, 72, 90, 142, 196, 206, 216, 227, 238, 254, 299]], [1225, [17, 37, 53, 57, 101, 122, 133, 144, 159, 174]], [1330, [16, 23, 37, 67, 77, 97, 118, 123, 132, 139]], [1334, [3, 26, 50, 63, 83, 91, 102, 114, 133, 149, 157, 163, 172, 187]], [1386, [4, 10, 33, 46, 52, 70, 140, 150, 162]], [1391, [83, 86, 104, 133, 166, 182]], [1416, [5, 9, 42, 50, 70, 100, 105, 134, 137, 152, 170, 172, 197, 202, 220]], [1446, [42, 52, 63]], [1497, [173]], [1686, [14]], [1735, [28]], [2482, [35, 36, 51, 67, 82, 105, 136, 139, 158, 166, 186]], [2497, [34]], [2585, [60, 63, 72, 80, 97, 111, 118, 153]], [2659, [108, 412]], [2791, [11]], [3234, [12, 32, 68, 102]], [3416, [11, 33, 43, 55, 67]], [3435, [96, 249]], [4894, [59]], [5172, [14]], [5256, [141]], [5330, [49]], [5595, [94]], [5596, [4, 46, 54, 68, 82, 116, 134, 160, 216, 220, 240, 258, 305, 314, 343, 350, 359, 381]], [5903, [275]], [5916, [8, 212, 367, 371]], [5954, [12]]]]\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "def load_index(path, compression_type):\n",
    "    f = open(path)\n",
    "    index = {}\n",
    "    if compression_type == \"no-compression\":\n",
    "        index = json.load(f)\n",
    "    if compression_type == \"variable-byte\":\n",
    "        data = json.load(f)\n",
    "        data = ast.literal_eval(data)\n",
    "        for term in data:\n",
    "            index[term] = []\n",
    "            index[term].append([])\n",
    "            index[term].append([])\n",
    "            for i in range(len(data[term][0][0])):\n",
    "                index[term][0].append([data[term][0][0][i]])\n",
    "                index[term][0][i].append(vb_decode(bytes.fromhex(data[term][0][1][i])))\n",
    "            for j in range(len(data[term][1][0])):\n",
    "                index[term][1].append([data[term][1][0][j]])\n",
    "                index[term][1][j].append(vb_decode(bytes.fromhex(data[term][1][1][j])))\n",
    "    if compression_type == \"gamma-code\":\n",
    "        data = json.load(f)\n",
    "        data = ast.literal_eval(data)\n",
    "        for term in data:\n",
    "            index[term] = []\n",
    "            index[term].append([])\n",
    "            index[term].append([])\n",
    "            for i in range(len(data[term][0][0])):\n",
    "                index[term][0].append([data[term][0][0][i]])\n",
    "                index[term][0][i].append(gamma_decode(bin(data[term][0][1][i])[2:]))\n",
    "            for j in range(len(data[term][1][0])):\n",
    "                index[term][1].append([data[term][1][0][j]])\n",
    "                index[term][1][j].append(gamma_decode(bin(data[term][1][1][j])[2:]))\n",
    "            \n",
    "    return index\n",
    "\n",
    "data = load_index(\"gamma_compressed_index.json\", \"gamma-code\")\n",
    "print(data[\"holm\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4PBbFcmpXFKD"
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h1>\n",
    "    <b>اصلاح پرسمان (۱۵ نمره امتیازی)</b>\n",
    "    </h1>\n",
    "    در صورتی که پرسمان ورودی دارای غلط املایی باشد یا به عبارتی لغاتی از آن در لغت‌نامه موجود نباشد، لازم است که با جستجوی لغت‌های احتمالی و انتخاب بهترین لغت به ادامه‌ی جستجو با پرسمان اصلاح شده پرداخته شود. برای اینکار ابتدا باید با روش bigram و معیار jaccard نزدیک‌ترین لغات به لغت با غلط املایی را پیدا کنید. سپس با استفاده از معیار edit distance بهترین لغت را از میان آن‌ها بیابید.<br>\n",
    "    نیازی به ذخیره‌سازی و فشرده‌سازی نمایه بایگرم نیست. همچنین می‌توانید از کد آماده برای محاسبه edit distance استفاده کنید.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "id": "qMZTstsbL1G3"
   },
   "outputs": [],
   "source": [
    "bigram_set = {}\n",
    "cnt_bigrams_in_term = {}\n",
    "\n",
    "def get_bigrams(term):\n",
    "    bigrams = {}\n",
    "    for i in range(len(term) - 1):\n",
    "        bigram = term[i:i + 2]\n",
    "        if not bigram in bigrams:\n",
    "            bigrams[bigram] = True\n",
    "    return list(bigrams.keys())\n",
    "\n",
    "def get_terms_with_bigram(bigram):\n",
    "    return bigram_set[bigram]\n",
    "\n",
    "def bigram_index_for_a_doc(doc):\n",
    "    all_terms = []\n",
    "    for header, term_list in doc.items():\n",
    "        all_terms += [(term, header, i) for i, term in enumerate(term_list)]\n",
    "    for (term, header, i) in all_terms:\n",
    "        if term in cnt_bigrams_in_term:\n",
    "            continue\n",
    "        bigrams = get_bigrams(term)\n",
    "        cnt_bigrams_in_term[term] = len(bigrams)\n",
    "        for bigram in bigrams:\n",
    "            if not bigram in bigram_set:\n",
    "                bigram_set[bigram] = []\n",
    "            bigram_set[bigram] += [term]\n",
    "\n",
    "def create_bigram_index():\n",
    "    for doc in sim_dic:\n",
    "        bigram_index_for_a_doc(doc)\n",
    "    return bigram_set\n",
    "\n",
    "bigrams = create_bigram_index()\n",
    "# print(bigrams[\"di\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "sL3E8C-nL1yG",
    "outputId": "d4da236e-728c-4419-e80e-e8a36bd4e85d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the dentures of sherlock holmes\n"
     ]
    }
   ],
   "source": [
    "JACCARD_N = 100\n",
    "\n",
    "\n",
    "def calc_jaccard(term1, term2):\n",
    "    bigrams1 = set(get_bigrams(term1))\n",
    "    bigrams2 = set(get_bigrams(term2))\n",
    "    return len(bigrams1 & bigrams2) / len(bigrams1 | bigrams2)\n",
    "\n",
    "\n",
    "def get_jaccard_nearests(term):\n",
    "    cnt_common_bigram_in_term = {}\n",
    "    bigrams = get_bigrams(term)\n",
    "    for bigram in bigrams:\n",
    "        if not bigram in bigram_set:\n",
    "            continue\n",
    "        for term2 in bigram_set[bigram]:\n",
    "            if not term2 in cnt_common_bigram_in_term:\n",
    "                cnt_common_bigram_in_term[term2] = 0\n",
    "            cnt_common_bigram_in_term[term2] += 1\n",
    "    ranking = []\n",
    "    for term2, cnt_intersect in cnt_common_bigram_in_term.items():\n",
    "        cnt_union = cnt_bigrams_in_term[term2] + len(bigrams) - cnt_intersect\n",
    "        ranking += [((cnt_intersect / cnt_union), term2)]\n",
    "    ranking.sort()\n",
    "    res = []\n",
    "    for i in range(max(0, len(ranking) - JACCARD_N), len(ranking)):\n",
    "        res += [ranking[i][1]]\n",
    "    res.reverse()\n",
    "    return res\n",
    "\n",
    "\n",
    "def calc_edit_distance(term1, term2):\n",
    "    dp = [i for i in range(len(term2) + 1)]\n",
    "    for i in range(1, len(term1) + 1):\n",
    "        dp2 = []\n",
    "        for j in range(len(term2) + 1):\n",
    "            dp2 += [int(1e9)]\n",
    "            if j == 0:\n",
    "                dp2[j] = min(dp2[j], i)\n",
    "                continue\n",
    "            if term1[i - 1] == term2[j - 1]:\n",
    "                dp2[j] = min(dp2[j], dp[j - 1])\n",
    "            dp2[j] = min(dp2[j], dp[j - 1] + 1)\n",
    "            dp2[j] = min(dp2[j], dp2[j - 1] + 1)\n",
    "            dp2[j] = min(dp2[j], dp[j] + 1)\n",
    "        dp = dp2\n",
    "    return dp[len(term2)]\n",
    "\n",
    "\n",
    "def correct_term(term):\n",
    "    if term in cnt_bigrams_in_term:\n",
    "        return term\n",
    "    nearest_terms = get_jaccard_nearests(term)\n",
    "    if len(nearest_terms) == 0:\n",
    "        return term\n",
    "    best_term = \"\"\n",
    "    best_edit_distance = int(1e9)\n",
    "    for term2 in nearest_terms:\n",
    "        edit_distance = calc_edit_distance(term, term2)\n",
    "        if edit_distance == best_edit_distance:\n",
    "            if term2 in stop_words.keys():\n",
    "                best_edit_distance = edit_distance\n",
    "                best_term = term2\n",
    "        elif edit_distance < best_edit_distance:\n",
    "            best_edit_distance = edit_distance\n",
    "            best_term = term2\n",
    "            \n",
    "    return best_term\n",
    "\n",
    "\n",
    "def get_corrected_text(raw_text):\n",
    "    query = raw_text.split()\n",
    "    new_terms = []\n",
    "    for term in query:\n",
    "        new_terms += [correct_term(term)]\n",
    "    ans = \"\"\n",
    "    for i in range(len(new_terms)):\n",
    "        if i == len(new_terms) - 1:\n",
    "            ans += new_terms[i]\n",
    "        else:\n",
    "            ans += new_terms[i] + \" \"\n",
    "    return ans\n",
    "print(get_corrected_text(\"the adevnture of herlock holmes\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-BWhdHJbs1zy"
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h1>\n",
    "    <b>جستجو و بازیابی اسناد (۲۵ نمره)</b>\n",
    "    </h1>\n",
    "    در این بخش جستجو ترتیب‌دار در فضای برداری tf-idf به روش <b>lnn-ltn</b> انجام می‌شود. یک پرسمان title و یک پرسمان plot ورودی گرفته شده و هر کدام در بخش مربوطه از اسناد جستجو می‌شوند. امتیاز نهایی هر سند برابر با جمع وزن‌دار امتیاز به دست آمده از جستجو در بخش title و plot است. به این صورت که وزن plot واحد در نظر گرفته شده و وزن تایتل به عنوان ورودی داده می‌شود.<br>\n",
    "    در نهایت اسناد برتر را نمایش دهید. تعداد حداکثر اسناد برتر نیز به عنوان ورودی داده می‌شود.<br><br>\n",
    "    نکته بسیار مهم نحوه نمایش اسناد انتخابی است. برای نمایش هر سند علاوه بر استفاده از شناسه و تیتر، یک هایلایت برای آن درست کنید. به این معنا که کلمات موجود در پرسمان را که باعث انتخاب سند شده‌اند به همراه ۲-۳ ترم قبل و بعد از آن به عنوان هایلایت آن سند نمایش دهید. اینگونه کاربر می‌تواند خیلی سریع دلیل بازیابی اسناد توسط سامانه را متوجه شود. مشابه کاری که سرچ گوگل انجام می‌دهد و ۲-۳ خط مربوطه را زیر وبسایت‌های پیشنهادی نمایش می‌دهد. طبیعتا راه حل بهینه برای این‌کار استفاده از قابلیت‌های نمایه جایگاهی می‌باشد.<br>\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R0R2i_O4Gev_",
    "outputId": "7f57be29-eec0-489e-dd61-74d67c923106"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4, 'the call of the wild', \"the most <b>popular</b> among the biograph company's ... in films, <b>florence</b> lawrence was already the ... \"], [4864, 'wild things', 'a <b>popular</b> miami area high school ... wealthy and <b>popular</b> girl named kelly van ... '], [2238, 'davy crockett, king of the wild frontier', 'becomes a <b>popular</b> member of the house ... ']]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import itertools\n",
    "import copy\n",
    "def calc_tf(strt):        \n",
    "    str2 = []\n",
    "    for i in strt:\n",
    "        if i not in str2:\n",
    "            str2.append(i) \n",
    "    res = {}     \n",
    "    for i in range(0, len(str2)):\n",
    "        res[str2[i]] = strt.count(str2[i])\n",
    "        if res[str2[i]] != 0:  \n",
    "            res[str2[i]] = 1 + math.log(res[str2[i]], 10)\n",
    "    return res  \n",
    "\n",
    "def calc_df(term, zone):\n",
    "    length = 0\n",
    "    for name in index:\n",
    "        if term == name:\n",
    "            if zone == \"title\":\n",
    "                length = len(index[term][0])\n",
    "            if zone == \"plot\":\n",
    "                length = len(index[term][1])\n",
    "    return length\n",
    "\n",
    "def calc_df_dic(ptdic, zone):\n",
    "    res = {}\n",
    "    for term in ptdic:\n",
    "        if calc_df(term, zone) == 0:\n",
    "            res[term] = 0\n",
    "        else:\n",
    "            res[term] = math.log(6000/calc_df(term, zone), 10)\n",
    "    return res\n",
    "\n",
    "def search(title_query, plot_query, title_weight, max_result_count=10):\n",
    "    global index\n",
    "    global stop_words\n",
    "    docs_scores = {}\n",
    "    title_query = title_query.lower().split()\n",
    "    plot_query = plot_query.lower().split()\n",
    "    valid_title = []\n",
    "    valid_plot = []\n",
    "    for token in title_query:\n",
    "        norm_token = re.sub(r'[^\\w\\s]', '', token.lower())\n",
    "        if norm_token:\n",
    "                valid_title.append(stem(lemma(norm_token)))\n",
    "    for token in plot_query:\n",
    "        norm_token = re.sub(r'[^\\w\\s]', '', token.lower())\n",
    "        if norm_token:\n",
    "                valid_plot.append(stem(lemma(norm_token)))\n",
    "    for stw in stop_words.keys():\n",
    "        valid_title[:] = (value for value in valid_title if value != stw)\n",
    "        valid_plot[:] = (value for value in valid_plot if value != stw)\n",
    "    ###### queries\n",
    "    title_tf = calc_tf(valid_title)\n",
    "    plot_tf = calc_tf(valid_plot)\n",
    "    title_idf = calc_df_dic(title_tf, \"title\")\n",
    "    plot_idf = calc_df_dic(plot_tf, \"plot\")\n",
    "    for term in title_tf:\n",
    "        title_tf[term] = title_tf[term]*title_idf[term]\n",
    "    for termm in plot_tf:\n",
    "        plot_tf[termm] = plot_tf[termm]*plot_idf[termm]\n",
    "    ###### docs\n",
    "    doc_num = 0\n",
    "    for doc in prepared_docs:\n",
    "        doc_title_tf = {}\n",
    "        doc_plot_tf = {}\n",
    "        for term in title_tf:\n",
    "            doc_title_tf[term] = 0\n",
    "        for term1 in plot_tf:\n",
    "            doc_plot_tf[term1] = 0\n",
    "        #title & plot\n",
    "        for term2 in doc[\"title\"]:\n",
    "            if term2 in doc_title_tf.keys():\n",
    "                doc_title_tf[term2] += 1\n",
    "        for term3 in doc[\"plot\"]:\n",
    "            if term3 in doc_plot_tf.keys():\n",
    "                doc_plot_tf[term3] += 1\n",
    "        for term4 in doc_title_tf:\n",
    "            if doc_title_tf[term4] != 0:\n",
    "                doc_title_tf[term4] = 1+ math.log(doc_title_tf[term4], 10)\n",
    "        for term5 in doc_plot_tf:\n",
    "            if doc_plot_tf[term5] != 0:\n",
    "                doc_plot_tf[term5] = 1+ math.log(doc_plot_tf[term5], 10)\n",
    "        #title_score\n",
    "        title_score = 0\n",
    "        for term6 in doc_title_tf:\n",
    "            title_score +=  doc_title_tf[term6]*title_tf[term6]\n",
    "        #plot_score\n",
    "        plot_score = 0\n",
    "        for term7 in doc_plot_tf:\n",
    "            plot_score +=  doc_plot_tf[term7]*plot_tf[term7]\n",
    "        doc_num += 1\n",
    "        docs_scores[doc_num] = title_score * title_weight + plot_score\n",
    "    sorted_by_score = sorted(docs_scores.items(), key=lambda item: -item[1])\n",
    "    selected_docs = [i for i, j in sorted_by_score][0:max_result_count]\n",
    "#     print(selected_docs)\n",
    "    selected_titles = []\n",
    "    for i in selected_docs:\n",
    "        strr = \"\"\n",
    "        for j in sim_dic[i-1][\"title\"]:\n",
    "            strr += j + \" \"\n",
    "        selected_titles.append(strr.rstrip())\n",
    "    answer = []\n",
    "    sorted_plot_idf = dict(sorted(plot_idf.items(), key=lambda item: item[1]))\n",
    "    for i in range(max_result_count):\n",
    "        highlight = \"\"\n",
    "        for term in dict(itertools.islice(sorted_plot_idf.items(), 4)):\n",
    "            for j in range(len(sim_dic[selected_docs[i]-1][\"plot\"])):\n",
    "                if stem(lemma(sim_dic[selected_docs[i]-1][\"plot\"][j])) == term:\n",
    "                    for k in range(j-2, j+5):\n",
    "                        if k >= 0 and k<len(sim_dic[selected_docs[i]-1][\"plot\"]):\n",
    "                            if k == j:\n",
    "                                highlight += \"<b>\" + sim_dic[selected_docs[i]-1][\"plot\"][k] + \"</b> \"\n",
    "                            else:\n",
    "                                highlight += sim_dic[selected_docs[i]-1][\"plot\"][k] + \" \"\n",
    "                    highlight += \"... \"\n",
    "        answer.append([selected_docs[i], selected_titles[i], highlight])\n",
    "    return answer\n",
    "\n",
    "print(search(\"wild\", \"Florence popular\", 10, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gBig369C6wSC"
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h1>\n",
    "    <b>ارزیابی عملکرد سامانه (۱۰ نمره)</b>\n",
    "    </h1>\n",
    "    همانطور که در بخش مربوط به مجموعه دادگان گفته شد، تعدادی پرسمان نمونه به همراه اسناد مورد نظر برای آن‌ها در اختیار شما قرار گرفته است. در هر مورد اطلاعات لازم برای ایجاد یک پرسمان ذکر شده است. مطابق آن‌ها پرسمان خود را ایجاد کنید. نتایج به دست آمده از هر پرسمان را به عنوان predicted results آن پرسمان در نظر بگیرید. همچنین در هر مورد لیستی از شناسه‌ها وجود دارد. این لیست را به عنوان actual results در نظر بگیرید. <br>\n",
    "    معیار‌های زیر را پیاده‌سازی کنید (بدون استفاده از کد آماده) و نتیجه این معیارها را روی مجموعه‌ی actual و predicted گزارش کنید. دقت کنید که به ازای هر پرسمان باید تمام معیارها را در قالب یک جدول گزارش کنید.<br> دقت کنید که ۳ پرسمان آخر فقط برای افرادی که بخش spell correction را پیاده کرد‌ه‌اند نتیجه مطلوب دارد.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aPfEVezQIsmb",
    "outputId": "28b08705-5d4f-47e6-baec-3372f3e3293c"
   },
   "outputs": [],
   "source": [
    "def precision(actual, predicted):\n",
    "    tp = len([element for element in actual if element in predicted])\n",
    "    tp_tn = len(predicted)\n",
    "    precision = 0\n",
    "    if tp_tn != 0:\n",
    "        precision = tp/tp_tn\n",
    "    return precision\n",
    "# precision([1,2], [1, 4, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "is4nfSw1LQoQ",
    "outputId": "cc2a4388-464b-4d67-cb86-32617e3e526d"
   },
   "outputs": [],
   "source": [
    "def recall(actual, predicted):\n",
    "    tp = [element for element in actual if element in predicted]\n",
    "    fn = 0\n",
    "    for element in actual:\n",
    "        if not element in tp:\n",
    "            fn += 1\n",
    "    recall = len(tp)/(len(tp)+fn)\n",
    "    return recall\n",
    "# recall([1,2], [1, 4, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eYHYwRgTLVRA",
    "outputId": "d749daf0-102d-4855-e292-21dcd137d6be"
   },
   "outputs": [],
   "source": [
    "def f1_score(actual, predicted):\n",
    "    prec = precision(actual, predicted)\n",
    "    rec = recall(actual, predicted)\n",
    "    f1_score = (2*prec*rec)/(prec+rec)\n",
    "    return f1_score\n",
    "\n",
    "# f1_score([1,2], [1, 4, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bZg-_PUPLZ31",
    "outputId": "e85b9efa-3b9b-470e-8b93-f474585da6c9"
   },
   "outputs": [],
   "source": [
    "def ap_score(actual, predicted):\n",
    "    true = 0\n",
    "    result = 0\n",
    "    for i in range(len(predicted)):\n",
    "        if predicted[i] in actual:\n",
    "            true += 1\n",
    "            result += true/(i+1)\n",
    "    result = result/len(actual)\n",
    "    return result\n",
    "\n",
    "def map_score(ap_scores):\n",
    "    result = 0\n",
    "    for i in ap_scores:\n",
    "        result += i\n",
    "    result = result/len(ap_scores)\n",
    "    return result\n",
    "\n",
    "# print(ap_score([1,2], [1, 4, 5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OgOKEbMNLf-r",
    "outputId": "b38f683a-7224-411b-efb4-8fe7ce4eb328"
   },
   "outputs": [],
   "source": [
    "def ndcg_score(actual, predicted):\n",
    "    actual_scores = []\n",
    "    predicted_scores = []\n",
    "    for i in range(len(predicted)):\n",
    "        predicted_scores.append(0)\n",
    "    length = len(actual)\n",
    "    for i in range(length):\n",
    "        actual_scores.append(length - i)\n",
    "    for k in range(len(predicted)):\n",
    "        for j in range(length):\n",
    "            if predicted[k] == actual[j]:\n",
    "                predicted_scores[k] = actual_scores[j]\n",
    "    \n",
    "    dcg_pred = 0\n",
    "    if len(predicted_scores) != 0:\n",
    "        dcg_pred += predicted_scores[0]\n",
    "    for i in range(1, len(predicted_scores)):\n",
    "        dcg_pred += predicted_scores[i]/math.log(i+1, 2)\n",
    "#     print(ideal_list)\n",
    "    dcg_actual = 0\n",
    "    if len(actual_scores) != 0:\n",
    "        dcg_actual += actual_scores[0]\n",
    "    for j in range(1, len(actual_scores)):\n",
    "        dcg_actual += actual_scores[j]/math.log(j+1, 2)\n",
    "    ndcg_score = dcg_pred/dcg_actual\n",
    "    return ndcg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query 1\n",
      "Actual: [626, 1812, 3297, 111, 74, 4213, 3296, 4507, 2760, 2039]\n",
      "Predicted: [626, 1812, 3297, 111, 74, 4213, 3296, 4507, 2760, 3286]\n",
      "Scores: {'Precision': 0.9, 'Recall': 0.9, 'F1score': 0.9, 'Average Precision': 0.9, 'NDCG Score': 0.99}\n",
      "Results: \n",
      "[[626, 'treasure island', \"cooper) and <b>his</b> mother (dorothy peterson) run ... map that <b>his</b> friend dr. livesey (otto ... beery) and <b>his</b> cronies. even though bones ... silver and <b>his</b> cohorts. then, the night ... stone) and <b>his</b> loyal men flee to ... to break <b>his</b> word to silver not ... hidden in <b>his</b> cave, and with silver ... silver as <b>his</b> prisoner. unable to stand ... and let <b>his</b> friend be hanged, jim ... money, jim <b>finds</b> a map that his ... he has <b>found</b> flint's treasure. meanwhile, smollett ... when they <b>find</b> it, it is empty. ... treasure and <b>kill</b> smollett's men. jim goes ... want to <b>kill</b> jim, but silver protects ... the next <b>day</b> the pirates search for ... \"], [1812, 'treasure island', \"lives with <b>his</b> mother in a tiny ... smollett and <b>his</b> ship, the hispaniola, bringing ... bringing along <b>his</b> friend dr. livesey as ... rest of <b>his</b> men, taking jim as ... hispaniola, arms <b>his</b> men with muskets and ... silver calls <b>his</b> men to attack. the ... slowed by <b>his</b> wound, it takes him ... to tend <b>his</b> wound, but the man ... him as <b>his</b> men wake up. merry ... map, which <b>his</b> men believe is with ... carry out <b>his</b> threat and drops the ... bones is <b>found</b> dead at the inn, ... spot. silver <b>finds</b> the map on him ... until they <b>find</b> out that the treasure ... but is <b>killed</b> by the boy's pistol. ... manages to <b>kill</b> three of them before ... \"], [3297, 'treasure island', 'pub with <b>his</b> mother (maria rohm). when ... jim gets <b>his</b> hands on a map ... has spent <b>his</b> time on the island ... treasure. with <b>his</b> help jim, the squire, ... the pirates, <b>killing</b> most of them in ... '], [111, 'treasure island', \"hawkins helps <b>his</b> widowed mother run the ... over to <b>his</b> mother's friends, dr. livesey ... silver and <b>his</b> gang, but the pirates ... flint, jim <b>finds</b> the map and turns ... the others <b>find</b> flint's treasure through the ... bones is <b>killed</b> at the inn by ... \"], [74, 'treasure island', '(carpenter) and <b>his</b> mother operate the admiral ... jim and <b>his</b> gang and through ben ... package she <b>found</b> in a chest that ... out to <b>find</b> the gold. long john ... (sargent) they <b>find</b> the treasure. just as ... and the <b>killing</b> off of long john ... '], [4213, 'treasure island', 'journey to <b>find</b> the treasure, but pirates ... '], [3296, 'treasure island', ''], [4507, 'cutthroat island', \"give dawg <b>his</b> piece and escapes with ... reveals to <b>his</b> daughter the location of ... piece: on <b>his</b> scalp. after scalping her ... to write <b>his</b> books. the crew then ... island with <b>his</b> and morgan's piece, but ... and ainslee, <b>his</b> men and reed are ... royal to <b>find</b> a translator. there, they ... vows to <b>find</b> her, either to arrest ... shaw secretly <b>finds</b> the piece and keeps ... together, they <b>find</b> the gold, only for ... consciousness, shaw <b>finds</b> reed, who leads him ... mordechai is <b>killed</b> and morgan is shot, ... reed are <b>killed</b> by cannon fire. morgan ... treasure. morgan <b>kills</b> dawg with a cannon ... \"], [2760, 'mysterious island', 'sailors during <b>his</b> crusade. making use of ... living aboard <b>his</b> disabled submarine. nemo has ... results of <b>his</b> genetic experiments to enlarge ... end all <b>his</b> life. due to their ... him in <b>his</b> efforts to make his ... to make <b>his</b> achievements known to the ... underwater using <b>his</b> special \"shell\" air tanks, ... island they <b>find</b> themselves on. they discover ... exploring and <b>find</b> a herd of wild ... then they <b>find</b> two unconscious english ladies, ... the castaways <b>find</b> cover and protection in ... the men <b>find</b> a variety of useful ... the rifles <b>found</b> in the chest indicate ... he apparently <b>kills</b> it. later, as they ... was actually <b>killed</b> by a bullet none ... nemo is <b>killed</b> as the nautilus is ... are one <b>day</b> tending to the goats, ... '], [3286, 'the magnificent seven ride', \"adams rescues <b>his</b> old friend, former bounty ... recovering from <b>his</b> long ride across the ... toro and <b>his</b> men. having recently married ... and assumed <b>his</b> job as marshal, however, ... request of <b>his</b> wife arrila to release ... responsible for <b>his</b> actions. chris then meets ... to discuss <b>his</b> exploits, donavan joins his ... donavan joins <b>his</b> friends, brothers hank and ... to celebrate <b>his</b> release. goaded into action ... arrila, despite <b>his</b> injury, and agrees to ... pleading for <b>his</b> life, insists that he ... shock. continuing <b>his</b> search for donavan, chris ... and tells <b>his</b> friend he is badly ... toro and <b>his</b> more than forty men ... to join <b>his</b> posse. although suspicious and ... for destroying <b>his</b> home. realizing they have ... and chris <b>find</b> arrila's dead body, which ... chris to <b>find</b> donavan and exact revenge. ... border and <b>finds</b> jim with a group ... only to <b>find</b> themselves circling back toward ... ridge, but <b>find</b> the farmers dead, although ... church to <b>find</b> the handful of townswomen ... the home. <b>finding</b> de toro's woman there, ... trek would <b>kill</b> them. realizing that the ... elliott are <b>killed</b> and noah wounded as ... and chris <b>kills</b> him. dismayed by the ... reviving two <b>days</b> later, chris immediately goes ... the previous <b>day</b> declaring the women's husbands ... only a <b>day</b> to prepare before de ... the next <b>day</b> with the women's assistance ... \"]]\n",
      "#####################################################################\n",
      "query 2\n",
      "Actual: [5180, 3152]\n",
      "Predicted: [5180, 3152]\n",
      "Scores: {'Precision': 1.0, 'Recall': 1.0, 'F1score': 1.0, 'Average Precision': 1.0, 'NDCG Score': 1.0}\n",
      "Results: \n",
      "[[5180, 'the matrix revolutions', 'which neo <b>attempts</b> to destroy, but is ... offers to <b>stop</b> smith in exchange for ... the sentinels <b>stop</b> attacking zion. the machines ... '], [3152, 'colossus: the forbin project', \"communicate using <b>only</b> simple arithmetic. even more ... unmonitored privacy <b>only</b> when they are in ... that colossus's <b>only</b> real power resides in ... heights, but <b>only</b> under its absolute rule. ... me not <b>only</b> with respect and awe, ... last desperate <b>attempt</b> to regain human control, ... colossus. the <b>attempted</b> system overload during routine ... monitoring the <b>attempts</b> to disarm its missiles ... \"]]\n",
      "#####################################################################\n",
      "query 3\n",
      "Actual: [4643, 4471, 5122, 4836, 3661, 3517, 4105]\n",
      "Predicted: [4643, 4471, 5122, 4836, 3661, 3517, 4105]\n",
      "Scores: {'Precision': 1.0, 'Recall': 1.0, 'F1score': 1.0, 'Average Precision': 1.0, 'NDCG Score': 1.0}\n",
      "Results: \n",
      "[[4643, 'star trek: first contact', \"the day <b>before</b> humanity's first encounter with ... the enterprise <b>arrives</b> in time to assist ... the enterprise <b>arrives</b> hundreds of years in ... arrives in <b>time</b> to assist the crew ... have used <b>time</b> travel to change the ... 2063, the <b>day</b> before humanity's first encounter ... the next <b>day</b> the crew watches from ... \"], [4471, 'star trek generations', 'the refugees <b>before</b> their ships are destroyed, ... rescued just <b>before</b> the station is destroyed ... only minutes <b>before</b> soran launches the missile. ... a hillside <b>before</b> a shuttle arrives to ... have them <b>arrive</b> on veridian iii only ... a shuttle <b>arrives</b> to transport him to ... realm where <b>time</b> has no meaning and ... '], [5122, 'star trek nemesis', 'positronic matrix <b>before</b> he boarded the scimitar. ... '], [4836, 'star trek: insurrection', 'rejuvenated selves <b>before</b> returning to their previous ... '], [3661, 'star trek ii: the wrath of khan', \"logical one, <b>before</b> succumbing to his injuries. ... mccoy, kirk <b>arrives</b> in the engine room ... stalls for <b>time</b> and uses the reliant's ... nebula in <b>time</b> due to the ship's ... \"], [3517, 'star trek: the motion picture', 'without purpose. <b>before</b> transmitting all its information, ... commander spock <b>arrives</b> as a replacement science ... '], [4105, 'star trek v: the final frontier', 'the enterprise <b>before</b> kirk can be transported ... ']]\n",
      "#####################################################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query 4\n",
      "Actual: [4670, 4971, 5309, 1136, 4920, 5475, 2871, 4160, 4357, 794, 5749, 4754]\n",
      "Predicted: [4670, 4971, 5514, 5309, 4920, 1136, 4377, 5475, 4160, 4357, 794, 1824]\n",
      "Scores: {'Precision': 0.75, 'Recall': 0.75, 'F1score': 0.75, 'Average Precision': 0.63, 'NDCG Score': 0.87}\n",
      "Results: \n",
      "[[4670, '12 angry men', 'slum, murdered <b>his</b> father. the jury is ... one from <b>his</b> pocket), and the overall ... slum) changes <b>his</b> vote to \"not guilty.\" ... to retrieve <b>his</b> knife, also changes his ... also changes <b>his</b> vote. jurors 2 and ... also changes <b>his</b> vote just so that ... still maintains <b>his</b> vote, he states his ... he states <b>his</b> belief that despite all ... 12 changes <b>his</b> vote back to \"guilty\" ... 4 rub <b>his</b> nose (which was being ... irritated by <b>his</b> glasses), realizes that, like ... to present <b>his</b> arguments again. he goes ... would kill <b>his</b> own father (it was ... relationship with <b>his</b> son). he begins to ... plunged into <b>his</b> chest. juror 8 points ... is not <b>his</b> son, and juror 4 ... 4 pats <b>his</b> arm and says: \"let ... 3 with <b>his</b> coat in a show ... juror 8 <b>helps</b> the distraught juror 3 ... they are <b>deciding</b> is whether the defendant, ... 6 also <b>decide</b> to vote \"not guilty\" ... '], [4971, 'rules of engagement', 'u.s. marines. <b>his</b> act thereby saves the ... reminiscing about <b>his</b> years in uniform. as ... he continued <b>his</b> career as a jag ... to honor <b>his</b> service at a pre-retirement ... event is <b>his</b> old friend, colonel terry ... childers and <b>his</b> embarked meu are called ... kingsley) and <b>his</b> family to a waiting ... then orders <b>his</b> men to open fire ... to be <b>his</b> defense attorney at the ... knowing that <b>his</b> record as a jag ... else in <b>his</b> team can testify to ... him and <b>his</b> family during the evacuation. ... when giving <b>his</b> order. while defending his ... while defending <b>his</b> actions, childers loses his ... childers loses <b>his</b> temper while stating that ... lives of <b>his</b> men to appease the ... armed and <b>his</b> poor choice of words ... having disobeyed <b>his</b> order to just show ... just show <b>his</b> marines), but not guilty ... '], [5514, '88 minutes', 'way to <b>his</b> class, gramm receives a ... call to <b>his</b> secretary, shelly (amy brenneman), ... suspicious of <b>his</b> students, particularly mike stempt ... and on <b>his</b> car, which has been ... met by <b>his</b> teaching assistant, kim cummings ... one of <b>his</b> students, lauren douglas (leelee ... go to <b>his</b> condo, where a package ... tape of <b>his</b> kid sister, kate, crying ... someone accessed <b>his</b> secure files to obtain ... kim that <b>his</b> sister was killed decades ... alone in <b>his</b> apartment; the crime took ... her at <b>his</b> office. shelly arrives at ... that recorded <b>his</b> \"confession\". he shares knowing ... offers to <b>help</b> find the perpetrator. in ... crying for <b>help</b> before being murdered. gramm ... '], [5309, 'the exorcism of emily rose', 'appear during <b>his</b> allotted time, and bruner ... moore in <b>his</b> jail cell, where he ... and cartwright <b>help</b> moore, emily escapes from ... '], [4920, 'the boondock saints', 'rocco and <b>his</b> two allies. this leaves ... joe leaves <b>his</b> house, smecker arrives in ... to join <b>his</b> two sons in their ... on-scene anticipate <b>his</b> acquittal. the brothers and ... himself to <b>helping</b> conner and murphy. that ... decides to <b>help</b> the trio. later, the ... rocco and <b>decides</b> to do an independent ... priest, smecker <b>decides</b> to help the trio. ... father and <b>deciding</b> to join his two ... '], [1136, 'roxie hart', 'north) starts <b>his</b> new job as a ... together with <b>his</b> new colleague homer howard ... murdered, and <b>his</b> body was found in ... homer finishes <b>his</b> story and gets up. ... bar by <b>his</b> wife, roxie, who arrives ... decides to <b>help</b> roxie out. when the ... the jury <b>helps</b> her case tremendously. she ... room. billy <b>decides</b> they will use the ... finnegan, and <b>decides</b> to help roxie out. ... '], [4377, 'malice', 'college, while <b>his</b> wife teaches art to ... plumbing. with <b>his</b> propensity to bring home ... to clear <b>his</b> name. while at the ... monologue about <b>his</b> own infallibility as a ... andy that <b>his</b> semen sample indicated that ... riley protests <b>his</b> innocence, but tells andy ... all of <b>his</b> questions. riley refuses to ... named in <b>his</b> will as a potential ... sitting by <b>his</b> bedroom window was in ... boy and <b>his</b> mother return home; as ... '], [5475, 'rendition', \"chicago with <b>his</b> mother, his pregnant wife ... his mother, <b>his</b> pregnant wife isabella (reese ... officials against <b>his</b> will and sent to ... records of <b>his</b> being on the flight ... remain of <b>his</b> boarding the plane at ... guilt, but <b>his</b> corrupt boss, corrine whitman ... to board <b>his</b> plane, but she shows ... her office, <b>his</b> sympathetic secretary quietly tips ... consent of <b>his</b> superiors, freeman approaches the ... to risk <b>his</b> entire life, and that ... that of <b>his</b> family. the minister quotes ... with isabella, <b>his</b> son, and the family's ... inmate at <b>his</b> prison and later died. ... group until <b>his</b> friends are arrested at ... khalid and <b>his</b> brother together, showing that ... standing over <b>his</b> brother's corpse, some pictures ... to avenge <b>his</b> brother's death. realising that ... brother met <b>his</b> death at the hands ... pin of <b>his</b> detonator, he hesitates, and ... realizes that <b>his</b> daughter died trying to ... possibility of <b>his</b> guilt was enough to ... the viewer <b>decide</b> whether the possibility of ... \"], [4160, 'frankenstein unbound', \"buchanan and <b>his</b> team work to develop ... buchanan and <b>his</b> futuristic computer-controlled car reappear ... is on <b>his</b> own. he drives his ... he drives <b>his</b> car to frankenstein's workshop ... to use <b>his</b> knowledge of electricity to ... laser in <b>his</b> car. as the lightning ... buchanan makes <b>his</b> way to the nearby ... \"], [4357, 'body of evidence', 'is tying <b>his</b> arms behind his back ... arms behind <b>his</b> back using his own ... back using <b>his</b> own belt. carlson pushes ... bed, removes <b>his</b> underwear, and while he ... wax on <b>his</b> chest, stomach, and genitals, ... her in <b>his</b> will. the testimony of ... contributed to <b>his</b> death, casting a reasonable ... restaurant where <b>his</b> wife works, and she ... she told <b>his</b> wife. at first she ... '], [794, 'back door to heaven', 'has throughout <b>his</b> life with his school ... life with <b>his</b> school colleagues and teacher, ... and how <b>his</b> calm demeanour no matter ... school and <b>his</b> family cannot afford to ... this occasion. <b>his</b> teacher, miss williams, asks ... crime. although <b>his</b> teacher and the sheriff ... back to <b>his</b> home town. it happens ... returns to <b>his</b> family shack, to find ... frankie that <b>his</b> father had died and ... and that <b>his</b> mother was taken away ... returning to <b>his</b> digs, he finds a ... note from <b>his</b> two friends saying that ... few moments. <b>his</b> parting message to his ... message to <b>his</b> teacher and classmates is ... '], [1824, 'along the great divide', \"merrick and <b>his</b> two deputies rescue cattle ... to administer <b>his</b> own brand of justice. ... he sends <b>his</b> other son, dan, to ... to gather <b>his</b> ranch hands while he ... night at <b>his</b> home, as it is ... to regret <b>his</b> decision when keith's daughter ... roden and <b>his</b> men. in the ensuing ... by capturing <b>his</b> son dan. as they ... reveals that <b>his</b> unswerving devotion to duty ... it cost <b>his</b> father his life. he ... his father <b>his</b> life. he was a ... deputy to <b>his</b> marshal father, and refused ... quickly draws <b>his</b> gun, but merrick is ... out of <b>his</b> hand. now, he has ... drops from <b>his</b> horse. keith grabs his ... keith grabs <b>his</b> gun, but is unwilling ... goes for <b>his</b> rifle, keith kills him, ... he killed <b>his</b> own brother, dan draws ... dan draws <b>his</b> revolver and grabs ann ... shield. when <b>his</b> father approaches, dan kills ... just like <b>his</b> brother, by merrick. ... refused to <b>help</b> escort two prisoners. all ... gray, to <b>help</b> him escape by the ... leave, ann <b>decides</b> to go with them. ... ranchers, merrick <b>decides</b> to take an unexpected ... \"]]\n",
      "#####################################################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query 5\n",
      "Actual: [5903]\n",
      "Predicted: [5903]\n",
      "Scores: {'Precision': 1.0, 'Recall': 1.0, 'F1score': 1.0, 'Average Precision': 1.0, 'NDCG Score': 1.0}\n",
      "Results: \n",
      "[[5903, 'the fault in our stars', 'there hazel <b>meets</b> augustus waters, a charming ... arranged the <b>meeting</b> and their dinner without ... confesses his <b>love</b> for hazel. the following ... about his <b>love</b> for her. she lies ... begin to <b>fall</b> in love. augustus surprises ... ']]\n",
      "#####################################################################\n",
      "query 6\n",
      "Actual: [5625]\n",
      "Predicted: [5625]\n",
      "Scores: {'Precision': 1.0, 'Recall': 1.0, 'F1score': 1.0, 'Average Precision': 1.0, 'NDCG Score': 1.0}\n",
      "Results: \n",
      "[[5625, 'inception', 'corporate espionage <b>using</b> an experimental military technology ... promises to <b>use</b> his influence to clear ... their success) <b>uses</b> as an opportunity to ... will be <b>used</b> to awaken the other ... reality, cobb <b>used</b> a rudimentary form of ... object dreamers <b>use</b> to distinguish dreams from ... his home. <b>using</b> his totem—a spinning top ... extract valuable <b>information</b> through a shared dream ... an experimental <b>military</b> technology to infiltrate the ... of their <b>targets</b> and extract valuable information ... ']]\n",
      "#####################################################################\n",
      "query 7\n",
      "Actual: [5153]\n",
      "Predicted: [5153]\n",
      "Scores: {'Precision': 1.0, 'Recall': 1.0, 'F1score': 1.0, 'Average Precision': 1.0, 'NDCG Score': 1.0}\n",
      "Results: \n",
      "[[5153, '44 minutes: the north hollywood shoot-out', '']]\n",
      "#####################################################################\n",
      "MAP Score: 0.93\n"
     ]
    }
   ],
   "source": [
    "f = open(\"validation.json\")\n",
    "queries = json.load(f)\n",
    "num = 1\n",
    "queries_results = []\n",
    "aps = []\n",
    "for query in queries:\n",
    "    scores = {}\n",
    "    corrected_title_query = get_corrected_text(query[\"title_query\"])\n",
    "    corrected_plot_query = get_corrected_text(query[\"plot_query\"])\n",
    "    max_size = int(query[\"max_size\"])\n",
    "    title_weight = int(query[\"title_weight\"])\n",
    "    actual = query[\"doc_ids\"]\n",
    "    results = search(corrected_title_query, corrected_plot_query, title_weight, max_size)\n",
    "    predicted = []\n",
    "    for i in range(len(results)):\n",
    "        predicted.append(results[i][0])\n",
    "    final_results.append([actual, predicted])\n",
    "    print(\"query \" + str(num))\n",
    "    num += 1\n",
    "    print(\"Actual: \" + str(actual))\n",
    "    print(\"Predicted: \" + str(predicted))\n",
    "    scores [\"Precision\"] = float(\"{0:.2f}\".format(precision(actual, predicted)))\n",
    "    scores[\"Recall\"] = float(\"{0:.2f}\".format(recall(actual, predicted)))\n",
    "    scores[\"F1score\"] = float(\"{0:.2f}\".format(f1_score(actual, predicted)))\n",
    "    scores[\"Average Precision\"] = float(\"{0:.2f}\".format(ap_score(actual, predicted)))\n",
    "    aps.append(scores[\"Average Precision\"])\n",
    "    # ndcg score\n",
    "    scores[\"NDCG Score\"] = float(\"{0:.2f}\".format(ndcg_score(actual, predicted)))\n",
    "    final_results.append(scores)\n",
    "    print(\"Scores: \" + str(scores))\n",
    "    print(\"Results: \")\n",
    "    print(results)\n",
    "    print(\"#####################################################################\")\n",
    "\n",
    "# MAP\n",
    "map_sc = map_score(aps)\n",
    "print(\"MAP Score: \" + str(\"{0:.2f}\".format(map_sc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SHVlw0kVBUW4"
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h1>\n",
    "    <b>نکات پایانی</b>\n",
    "    </h1>\n",
    "    \n",
    "\n",
    "1.   سیستم را بهینه پیاده‌سازی کنید تا در زمان کمتری بارگذاری و نمایه‌سازی  انجام شود.\n",
    "2.   تمام قطعه‌ کدهای بالا باید توسط شما تکمیل شود. نوت‌بوک نهایی باید بدون خطا اجرا شود. اگر تمام کدهای استفاده شده در همین فایل نیست،‌ حتما آن‌ها را نیز به همراه نوت‌بوک در کوئرا آپلود کنید.\n",
    "3.    اسم توابع و نحوه ورودی گرفتن و خروجی دادن آن‌ها را تغییر ندهید. بقیه اجزای توابع صرفا برای شهود بیشتر شما نوشته شده‌اند و نیازی به نگه‌داری آن‌ها نیست.\n",
    "4.   در صورت امکان استفاده از کد آماده مشخصا این مورد برای بخش مربوطه ذکر شده است. اگر چیزی در این مورد ذکر نشده نمی‌توانید از کد آماده استفاده کنید.\n",
    "5.    فایل داده‌ها را در کوئرا آپلود نکنید.\n",
    "6.    فایل زیپ نهایی و فایل نوت‌بوک حتما به فرمت StudentNumber_phase1 نامگذاری شود.\n",
    "\n",
    "\n",
    "<b>سالم و موفق باشید.</b>\n",
    "\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "MIR_Phase1_v1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
